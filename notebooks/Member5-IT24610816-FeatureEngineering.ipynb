{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dd3d9a-8420-48f6-a070-01699145fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup (paths, imports) ---\n",
    "BASE_DIR  = r\"E:\\AIML\"\n",
    "DATA_PATH = rf\"E:\\AIML\\dataset\\heart_attack_prediction_dataset.csv\"\n",
    "EDA_DIR   = rf\"E:\\AIML\\results\\eda_visualizations\"\n",
    "OUT_DIR   = rf\"E:\\AIML\\results\\outputs\"\n",
    "\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, warnings\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception:\n",
    "    SMOTE = None  # Only needed for Member F\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.makedirs(EDA_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET = \"Heart Attack Risk\"\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa127987-f27f-4143-8659-07bccb1281d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test: (7010, 24) (1753, 24)\n"
     ]
    }
   ],
   "source": [
    "# --- Load & Split (before any fitting) ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "assert TARGET in df.columns, f\"TARGET '{TARGET}' not found. Columns: {list(df.columns)}\"\n",
    "\n",
    "# optional: drop ID-like columns\n",
    "id_like = {\"patient id\",\"id\",\"record id\"}\n",
    "drop_ids = [c for c in df.columns if c.strip().lower() in id_like]\n",
    "if drop_ids:\n",
    "    df = df.drop(columns=drop_ids)\n",
    "\n",
    "X = df.drop(columns=[TARGET]).copy()\n",
    "y = df[TARGET].values\n",
    "strat = y if pd.Series(y).nunique() <= 20 else None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=strat\n",
    ")\n",
    "print(\"Train/Test:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1604a7c-5027-4737-b947-149fa0f66639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical: ['Sex', 'Blood Pressure', 'Diabetes', 'Family History', 'Smoking', 'Obesity', 'Alcohol Consumption', 'Diet', 'Previous Heart Problems', 'Medication Use', 'Stress Level', 'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Country', 'Continent', 'Hemisphere']\n",
      "Numeric: ['Age', 'Cholesterol', 'Heart Rate', 'Exercise Hours Per Week', 'Sedentary Hours Per Day', 'Income', 'BMI', 'Triglycerides']\n"
     ]
    }
   ],
   "source": [
    "# --- Detect categorical vs numeric ---\n",
    "LOW_CARD_AS_CAT = 12\n",
    "categorical_cols = []\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype == \"object\":\n",
    "        categorical_cols.append(c)\n",
    "    else:\n",
    "        if X_train[c].nunique(dropna=True) <= LOW_CARD_AS_CAT:\n",
    "            categorical_cols.append(c)\n",
    "numeric_cols = [c for c in X_train.select_dtypes(include=\"number\").columns if c not in categorical_cols]\n",
    "print(\"Categorical:\", categorical_cols)\n",
    "print(\"Numeric:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ab13cf-a234-4473-8b35-9937ab2ea1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing locally (median numeric, most_frequent categorical)...\n",
      "Imputed shapes: (7010, 24) (1753, 24)\n"
     ]
    }
   ],
   "source": [
    "# --- Pre-req: Impute (load if available; else compute) ---\n",
    "import os\n",
    "\n",
    "train_imp_path = os.path.join(OUT_DIR, \"X_train_imputed.csv\")\n",
    "test_imp_path  = os.path.join(OUT_DIR, \"X_test_imputed.csv\")\n",
    "\n",
    "if os.path.exists(train_imp_path) and os.path.exists(test_imp_path):\n",
    "    print(\"Loading imputed splits from outputs/...\")\n",
    "    X_train_imp = pd.read_csv(train_imp_path).drop(columns=[TARGET], errors=\"ignore\")\n",
    "    X_test_imp  = pd.read_csv(test_imp_path).drop(columns=[TARGET], errors=\"ignore\")\n",
    "else:\n",
    "    print(\"Imputing locally (median numeric, most_frequent categorical)...\")\n",
    "    num_imputer = SimpleImputer(strategy=\"median\")\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    X_train_num = num_imputer.fit_transform(X_train[numeric_cols]) if numeric_cols else None\n",
    "    X_test_num  = num_imputer.transform(X_test[numeric_cols])      if numeric_cols else None\n",
    "    X_train_cat = cat_imputer.fit_transform(X_train[categorical_cols]) if categorical_cols else None\n",
    "    X_test_cat  = cat_imputer.transform(X_test[categorical_cols])      if categorical_cols else None\n",
    "    tr_parts, te_parts = [], []\n",
    "    if X_train_num is not None:\n",
    "        tr_parts.append(pd.DataFrame(X_train_num, columns=numeric_cols, index=X_train.index))\n",
    "        te_parts.append(pd.DataFrame(X_test_num,  columns=numeric_cols, index=X_test.index))\n",
    "    if X_train_cat is not None:\n",
    "        tr_parts.append(pd.DataFrame(X_train_cat, columns=categorical_cols, index=X_train.index))\n",
    "        te_parts.append(pd.DataFrame(X_test_cat,  columns=categorical_cols, index=X_test.index))\n",
    "    X_train_imp = pd.concat(tr_parts, axis=1) if tr_parts else pd.DataFrame(index=X_train.index)\n",
    "    X_test_imp  = pd.concat(te_parts, axis=1) if te_parts else pd.DataFrame(index=X_test.index)\n",
    "\n",
    "print(\"Imputed shapes:\", X_train_imp.shape, X_test_imp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e38cafd-73e1-4453-a853-23ba7dc161a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric-only matrices -> TRAIN: (7010, 3646)  TEST: (1753, 3646)\n"
     ]
    }
   ],
   "source": [
    "# --- E1: Ensure numeric-only matrix (OHE non-numerics if any) ---\n",
    "# One-hot the non-numeric columns to make MI/PCA numeric-only\n",
    "non_num_cols = [c for c in X_train_imp.columns if c not in X_train.select_dtypes(include=\"number\").columns]\n",
    "if non_num_cols:\n",
    "    try:\n",
    "        ohe_fe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe_fe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    Xtr_ohe = ohe_fe.fit_transform(X_train_imp[non_num_cols].astype(\"category\"))\n",
    "    Xte_ohe = ohe_fe.transform(X_test_imp[non_num_cols].astype(\"category\"))\n",
    "    try:\n",
    "        ohe_cols = list(ohe_fe.get_feature_names_out(non_num_cols))\n",
    "    except AttributeError:\n",
    "        ohe_cols = [f\"{c}_{v}\" for c, cats in zip(non_num_cols, ohe_fe.categories_) for v in cats]\n",
    "    df_tr_ohe = pd.DataFrame(Xtr_ohe, columns=ohe_cols, index=X_train_imp.index)\n",
    "    df_te_ohe = pd.DataFrame(Xte_ohe, columns=ohe_cols, index=X_test_imp.index)\n",
    "else:\n",
    "    df_tr_ohe = pd.DataFrame(index=X_train_imp.index)\n",
    "    df_te_ohe = pd.DataFrame(index=X_test_imp.index)\n",
    "\n",
    "Xtr_num = X_train_imp.select_dtypes(include=\"number\").copy()\n",
    "Xte_num = X_test_imp.select_dtypes(include=\"number\").copy()\n",
    "\n",
    "X_train_FE = pd.concat([Xtr_num, df_tr_ohe], axis=1)\n",
    "X_test_FE  = pd.concat([Xte_num, df_te_ohe], axis=1)\n",
    "print(\"Numeric-only matrices -> TRAIN:\", X_train_FE.shape, \" TEST:\", X_test_FE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cda5ef-d86e-4f34-acba-9f3b177b9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E2: Mutual Information (Top-K bar) ---\n",
    "TOP_K_MI = 15\n",
    "mi_scores = mutual_info_classif(X_train_FE.values, y_train, discrete_features=False, random_state=RANDOM_STATE)\n",
    "mi_df = pd.DataFrame({\"feature\": X_train_FE.columns, \"mi\": mi_scores}).sort_values(\"mi\", ascending=False)\n",
    "\n",
    "k = min(TOP_K_MI, len(mi_df))\n",
    "topk = mi_df.head(k).iloc[::-1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(topk[\"feature\"], topk[\"mi\"])\n",
    "plt.title(f\"Top {k} Features by Mutual Information\")\n",
    "plt.xlabel(\"MI score\"); plt.ylabel(\"Feature\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(EDA_DIR, f\"E_mi_top_{k}.png\")); plt.show()\n",
    "\n",
    "mi_df.to_csv(os.path.join(OUT_DIR, \"E_feature_importance_MI.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610f506-be44-4237-946c-24f3cc291871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E3: PCA (fit on TRAIN) + plots + transform to ~95% variance ---\n",
    "pca = PCA(n_components=None, random_state=RANDOM_STATE).fit(X_train_FE.values)\n",
    "exp = pca.explained_variance_ratio_; cum = np.cumsum(exp)\n",
    "\n",
    "plt.figure(); plt.plot(range(1, len(exp)+1), exp, marker=\"o\")\n",
    "plt.title(\"PCA — Explained Variance Ratio per Component\")\n",
    "plt.xlabel(\"Principal Component\"); plt.ylabel(\"Explained variance ratio\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(EDA_DIR, \"E_pca_explained_variance.png\")); plt.show()\n",
    "\n",
    "plt.figure(); plt.plot(range(1, len(cum)+1), cum, marker=\"o\"); plt.axhline(0.95, linestyle=\"--\")\n",
    "plt.title(\"PCA — Cumulative Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\"); plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(EDA_DIR, \"E_pca_cumulative_variance.png\")); plt.show()\n",
    "\n",
    "n95 = int(np.argmax(cum >= 0.95) + 1)\n",
    "print(f\"~95% variance retained with {n95} PCs.\")\n",
    "\n",
    "pca95 = PCA(n_components=n95, random_state=RANDOM_STATE).fit(X_train_FE.values)\n",
    "X_train_pca = pca95.transform(X_train_FE.values)\n",
    "X_test_pca  = pca95.transform(X_test_FE.values)\n",
    "\n",
    "pd.DataFrame(X_train_pca, columns=[f\"PC{i}\" for i in range(1, n95+1)])\\\n",
    "  .to_csv(os.path.join(OUT_DIR, f\"E_X_train_PCA_{n95}.csv\"), index=False)\n",
    "pd.DataFrame(X_test_pca,  columns=[f\"PC{i}\" for i in range(1, n95+1)])\\\n",
    "  .to_csv(os.path.join(OUT_DIR,  f\"E_X_test_PCA_{n95}.csv\"),  index=False)\n",
    "print(\"Saved PCA CSVs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9313d97-5120-42f6-9ca5-7adc3ab3edb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
